{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "455c4e53",
      "metadata": {
        "id": "455c4e53"
      },
      "outputs": [],
      "source": [
        "####  Install Java e PySpark (run it only once)\n",
        "\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!pip install pyspark -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "affa0d8b",
      "metadata": {
        "id": "affa0d8b"
      },
      "outputs": [],
      "source": [
        "####  Java Environment\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d561c7b2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "d561c7b2",
        "outputId": "55e5cfca-5822-4337-c57f-fab4bf9a23e3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7bc815487190>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://69a17732e82f:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.5</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>FakeNewsFromDrive</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "#### Starting Spark Session\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"FakeNewsFromDrive\").getOrCreate()\n",
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e8f9e54",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1e8f9e54",
        "outputId": "08d232b0-0571-49b9-edc4-a499edb45ce1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#### Google drive for repository (ask Antonio if you don't know what to do locally) - Using this just to colab dev. It'll be adapted for Databricks delivery.\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e52312b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1e52312b",
        "outputId": "2660df16-0471-417e-8cde-b7efc34859c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using path: /content/drive/MyDrive/fake_news_project\n"
          ]
        }
      ],
      "source": [
        "project_path = \"/content/drive/MyDrive/fake_news_project\"\n",
        "print(f\"Using path: {project_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a476b887",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a476b887",
        "outputId": "6012480c-930b-412f-b95c-3dd3eb9c8730"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----+\n",
            "|                text|label|\n",
            "+--------------------+-----+\n",
            "|Donald Trump just...|    0|\n",
            "|House Intelligenc...|    0|\n",
            "|On Friday, it was...|    0|\n",
            "|On Christmas day,...|    0|\n",
            "|Pope Francis used...|    0|\n",
            "+--------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#### Reading and joining CSVs & setting fake or real label\n",
        "\n",
        "df_fake = spark.read.csv(f\"{project_path}/fake.csv\", header=True, inferSchema=True)\n",
        "df_real = spark.read.csv(f\"{project_path}/real.csv\", header=True, inferSchema=True)\n",
        "\n",
        "from pyspark.sql.functions import lit\n",
        "df_fake = df_fake.withColumn(\"label\", lit(0))\n",
        "df_real = df_real.withColumn(\"label\", lit(1))\n",
        "df = df_fake.unionByName(df_real).select(\"text\", \"label\").na.drop()\n",
        "\n",
        "df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d73dd4ba",
      "metadata": {
        "id": "d73dd4ba"
      },
      "outputs": [],
      "source": [
        "#### Preprocessing and split (train, test)\n",
        "\n",
        "from pyspark.sql.functions import lower, regexp_replace\n",
        "\n",
        "## removing everything that is not a-zA-Z + blanks. Then lowering text\n",
        "df = df.withColumn(\"text\", lower(regexp_replace(\"text\", \"[^a-zA-Z\\s]\", \"\")))\n",
        "\n",
        "train_data, test_data = df.randomSplit([0.8, 0.2], seed=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40f4a108",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40f4a108",
        "outputId": "de3bb7f4-3019-42d9-c785-038e3b6b603d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest AUC + Cross Validation: 0.9954\n"
          ]
        }
      ],
      "source": [
        "#### RandomForest with Cross Validation Pipeline\n",
        "\n",
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "\n",
        "## Text Mining\n",
        "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\") ## Tokenizing (text to words list)\n",
        "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\") ## Removing Stop Words from list\n",
        "tf = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\") ## Hashing vector\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\") ## weight adjustment\n",
        "\n",
        "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=20)\n",
        "\n",
        "pipeline = Pipeline(stages=[tokenizer, remover, tf, idf, rf])\n",
        "\n",
        "paramGrid = ParamGridBuilder() \\\n",
        "    .addGrid(tf.numFeatures, [1000, 5000]) \\\n",
        "    .addGrid(rf.maxDepth, [5, 10]) \\\n",
        "    .build()\n",
        "\n",
        "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
        "\n",
        "cv = CrossValidator(estimator=pipeline,\n",
        "                    estimatorParamMaps=paramGrid,\n",
        "                    evaluator=evaluator,\n",
        "                    numFolds=3)\n",
        "\n",
        "cv_model = cv.fit(train_data)\n",
        "predictions = cv_model.transform(test_data)\n",
        "\n",
        "auc = evaluator.evaluate(predictions)\n",
        "print(f\"Random Forest AUC + Cross Validation: {auc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### Accuracy evaluation\n",
        "\n",
        "correct_preds = predictions.filter(\"label = prediction\").count()\n",
        "total_preds = predictions.count()\n",
        "accuracy = correct_preds / total_preds\n",
        "print(f\"✅ Acuraccy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2PMdJOc05za",
        "outputId": "806333b1-9854-4790-b8c9-09624abb09e1"
      },
      "id": "J2PMdJOc05za",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Acuraccy: 0.9724\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### Saving best model\n",
        "\n",
        "## Google drive best_model path\n",
        "model_path = \"/content/drive/MyDrive/fake_news_project/best_model\"\n",
        "\n",
        "## Saving CV Best Model with best parameters\n",
        "cv_model.bestModel.write().overwrite().save(model_path)\n",
        "\n",
        "print(\"Best Model saved:\", model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MH1V2aB6t2_i",
        "outputId": "3e63c501-9ab2-4b1e-d695-1f275ff8ef1b"
      },
      "id": "MH1V2aB6t2_i",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Model saved: /content/drive/MyDrive/fake_news_project/best_model\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}